# -*- coding: utf-8 -*-
"""DETECTION BE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12aZwwHUSS7SRTC-ihJYMVH5G_XLBxXtg

PRE-PROCESSING
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/BE PROJECT/juliet_data.csv')

print(df.head())

df.shape

df.drop(columns=['Unnamed: 0', 'testcase_ID','filename'], inplace=True)

df.head(5)

print(df['bug'].tail(50).value_counts())

first_10_snippets = df['code'].tail(50)

snippet_lengths = first_10_snippets.apply(len)

average_length = snippet_lengths.mean()

print(average_length)

print(df['flaw'].value_counts()[:20])

import pandas as pd
import matplotlib.pyplot as plt
flaw_counts = df['flaw'].value_counts()[:20]

# Plotting
plt.figure(figsize=(10, 6))
flaw_counts.plot(kind='bar')
plt.title('Top 20 Flaws')
plt.xlabel('Flaw')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()



df['code'] = df['code'].str.replace(' ', '')

# Now, 'code' column in the DataFrame will have spaces removed
print(df)

print(df['code'][1])

import pandas as pd
df = df.sample(frac=1, random_state=42)
df = df.head(210366)

df.head()

import pandas as pd
import re

#Comments removed
comment_pattern = r'/\*[\s\S]*?\*/'

# Assuming 'df' is your DataFrame with a 'code' column
df['code_without_comments'] = df['code'].apply(lambda code_snippet: re.sub(comment_pattern, '', code_snippet))

# The 'code_without_comments' column now contains code snippets without comments

df.head()

print(df['code_without_comments'][2841])

import numpy as np
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense
from keras.optimizers import Adam

# Sample text data and corresponding labels
texts = df['code_without_comments']
labels = df['bug'] # 1 for positive, 0 for negative

# Tokenize the text data
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

# Pad sequences to a fixed length (adjust maxlen as needed)
maxlen = 10
X = pad_sequences(sequences, maxlen=maxlen)

# Convert labels to numpy array
y = np.array(labels)

# Define the CNN model
model = Sequential()
model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=maxlen))
model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification

# Compile the model
optimizer = Adam(learning_rate=0.001)
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

# Train the model
model.fit(X, y, epochs=10, batch_size=16)

model.save('/content/drive/MyDrive/BE PROJECT/cnn_b16_full.h5')

from keras.models import load_model

# Replace 'your_model.h5' with the path to your saved model file.
model = load_model('/content/drive/MyDrive/BE PROJECT/cnn_b16_full.h5')

# Now, the 'model' variable holds your loaded model, and you can use it for predictions or further training.

model.save("cnn_b16_full.h5")

input_code=["""
#include <stdio.h>

void foo(char buff[10]) {
    strcpy(buff, "Hello, World!");
}

int main() {
    char buff[10];
    foo(buff);
    printf("%s\n", buff);
    return 0;


"""]

# Example input code for testing


# Tokenize and pad the input code
input_sequences = tokenizer.texts_to_sequences(input_code)
input_padded = pad_sequences(input_sequences, maxlen=maxlen)

# Use the model to make predictions
predictions = model.predict(input_padded)

# Interpret the predictions
for i, code in enumerate(input_code):
    prediction = predictions[i][0]
    print(prediction)
    if prediction >= 0.5:
        print(f"Code:\n{code}\nPredicted: Positive (Bug)\n")
    else:
        print(f"Code:\n{code}\nPredicted: Negative (No Bug)\n")

vul_code=["""#include <stdio.h>
int main(int argc, char **argv)
{
char buf[8]; // buffer for eight characters
gets(buf); // read from stdio (sensitive function!)
printf("%s\n", buf); // print out data stored in buf
return 0; // 0 as return value
}
"""]

vul_code_1=["""
 headname));\t\\\n\t    (var);\t\t\t\t\t\t\t\\\n\t    (var) = TAILQ_PREV((var), headname, field))\n\n#define\tTAILQ_FOREACH_REVERSE_SAFE(var, head, headname, field, tvar)\t\\\n\tfor ((var) = TAILQ_LAST((head), headname);\t\t\t\\\n\t    (var) && ((tvar) = TAILQ_PREV((var), headname, field), 1);\t\\\n\t    (var) = (tvar))\n\n#define\tTAILQ_FOREACH_REVERSE_FROM_SAFE(var, head, headname, field, tvar) \\\n\tfor ((var) = ((var) ? (var) : TAILQ_LAST((head), headname));\t\\\n\t    (var) && ((tvar) = TAILQ_PREV((var), headname, field), 1);\t\\\n\t    (var) = (tvar))\n\n#define\tTAILQ_INIT(head) do {\t\t\t\t\t\t\\\n\tTAILQ_FIRST((head)) = NULL;\t\t\t\t\t\\\n\t(head)->tqh_last = &TAILQ_FIRST((head));\t\t\t\\\n\tQMD_TRACE_HEAD(head);\t\t\t\t\t\t\\\n} while (0)\n\n#define\tTAILQ_INSERT_AFTER(head, listelm, elm, field) do {\t\t\\\n\tQMD_TAILQ_CHECK_NEXT(listelm, field);\t\t\t\t\\\n\tif ((TAILQ_NEXT((elm), field) = TAILQ_NEXT((listelm), field)) != NULL)\\\n\t\tTAILQ_NEXT((elm), field)->field.tqe_prev = \t\t\\\n\t\t    &TAILQ_NEXT((elm), field);\t\t\t\t\\\n\telse {\t\t\t\t\t\t\t\t\\\n\t\t(head)->tqh_last = &TAILQ_NEXT((elm), field);\t\t\\\n\t\tQMD_TRACE_HEAD(head);\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tTAILQ_NEXT((listelm), field) = (elm);\t\t\t\t\\\n\t(elm)->field.tqe_prev = &TAILQ_NEXT((listelm), field);\t\t\\\n\tQMD_TRACE_ELEM(&(elm)->field);\t\t\t\t\t\\\n\tQMD_TRACE_ELEM(&(listelm)->field);\t\t\t\t\\\n} while (0)\n\n#define\tTAILQ_INSERT_BEFORE(listelm, elm, field) do {\t\t\t\\\n\tQMD_TAILQ_CHECK_PREV(listelm, field);\t\t\t\t\\\n\t(elm)->field.tqe_prev = (listelm)->field.tqe_prev;\t\t\\\n\tTAILQ_NEXT((elm), field) = (listelm);\t\t\t\t\\\n\t*(listelm)->field.tqe_prev = (elm);\t\t\t\t\\\n\t(listelm)->field.tqe_prev = &TAILQ_NEXT((elm), field);\t\t\\\n\tQMD_TRACE_ELEM(&(elm)->field);\t\t\t\t\t\\\n\tQMD_TRACE_ELEM(&(listelm)->field);\t\t\t\t\\\n} while (0)\n\n#define\tTAILQ_INSERT_HEAD(head, elm, field) do {\t\t\t\\\n\tQMD_TAILQ_CHECK_HEAD(head, field);\t\t\t\t\\\n\tif ((TAILQ_NEXT((elm), field) = TAILQ_FIRST((head))) != NULL)\t\\\n\t\tTAILQ_FIRST((head))->field.tqe_prev =\t\t\t\\\n\t\t    &TAILQ_NEXT((elm), field);\t\t\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t(head)->tqh_last = &TAILQ_NEXT((elm), field);\t\t\\\n\tTAILQ_FIRST((head)) = (elm);\t\t\t\t\t\\\n\t(elm)->field.tqe_prev = &TAILQ_FIRST((head));\t\t\t\\\n\tQMD_TRACE_HEAD(head);\t\t\t\t\t\t\\\n\tQMD_TRACE_ELEM(&(elm)->field);\t\t\t\t\t\\\n} while (0)\n\n#define\tTAILQ_INSERT_TAIL(head, elm, field) do {\t\t\t\\\n\tQMD_TAILQ_CHECK_TAIL(head, field);\t\t\t\t\\\n\tTAILQ_NEXT((elm), field) = NULL;\t\t\t\t\\\n\t(elm)->field.tqe_prev = (head)->tqh_last;\t\t\t\\\n\t*(head)->tqh_last = (elm);\t\t\t\t\t\\\n\t(head)->tqh_last = &TAILQ_NEXT((elm), field);\t\t\t\\\n\tQMD_TRACE_HEAD(head);\t\t\t\t\t\t\\\n\tQMD_TRACE_ELEM(&(elm)->field);\t\t\t\t\t\\\n} while (0)\n\n#define\tTAILQ_LAST(head, headname)\t\t\t\t\t\\\n\t((((struct headname *)((head)->tqh_last))->tqh_last))\n\n#define\tTAILQ_NEXT(elm, field) ((elm)->field.tqe_next)\n\n#define\tTAILQ_PREV(elm, headname, field)\t\t\t\t\\\n\t(*(((struct headname *)((elm)->field.tqe_prev))->tqh_last))\n\n#define\tTAILQ_REMOVE(head, elm, field) do {\t\t\t\t\\\n\tQMD_SAVELINK(oldnext, (elm)->field.tqe_next);\t\t\t\\\n\tQMD_SAVELINK(oldprev, (elm)->field.tqe_prev);\t\t\t\\\n\tQMD_TAILQ_CHECK_NEXT(elm, field);\t\t\t\t\\\n\tQMD_TAILQ_CHECK_PREV(elm, field);\t\t\t\t\\\n\tif ((TAILQ_NEXT((elm), field)) != NULL)\t\t\t\t\\\n\t\tTAILQ_NEXT((elm), field)->field.tqe_prev = \t\t\\\n\t\t    (elm)->field.tqe_prev;\t\t\t\t\\\n\telse {\t\t\t\t\t\t\t\t\\\n\t\t(head)->tqh_last = (elm)->field.tqe_prev;\t\t\\\n\t\tQMD_TRACE_HEAD(head);\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\t(elm)->field.tqe_prev = TAILQ_NEXT((elm), field);\t\t\\\n\tTRASHIT(*oldnext);\t\t\t\t\t\t\\\n\tTRASHIT(*oldprev);\t\t\t\t\t\t\\\n\tQMD_TRACE_ELEM(&(elm)->field);\t\t\t\t\t\\\n} while (0)\n\n#define TAILQ_SWAP(head1, head2, type, field) do {\t\t\t\\\n\tQUEUE_TYPEOF(type) *swap_first = (head1)->tqh_first;\t\t\\\n\tQUEUE_TYPEOF(type) **swap_last = (head1)->tqh_last;\t\t\\\n\t(head1)->tqh_first = (head2)->tqh_first;\t\t\t\\\n\t(head1)->tqh_last = (head2)->tqh_last;\t\t\t\t\\\n\t(head2)->tqh_first = swap_first;\t\t\t\t\\\n\t(head2)->tqh_last = swap_last;\t\t\t\t\t\\\n\tif ((swap_first = (head1)->tqh_first) != NULL)\t\t\t\\\n\t\tswap_first->field.tqe_prev = &(head1)->tqh_first;\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t(head1)->tqh_last = &(head1)->tqh_first;\t\t\\\n\tif ((swap_first = (head2)->tqh_first) != NULL)\t\t\t\\\n\t\tswap_first->field.tqe_prev = &(head2)->tqh_first;\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t(head2)->tqh_last = &(head2)->tqh_first;\t\t\\\n} while (0)\n\n#endif \n'
"""]

# Example input code for testing


# Tokenize and pad the input code
input_sequences = tokenizer.texts_to_sequences(vul_code_1)
input_padded = pad_sequences(input_sequences, maxlen=maxlen)

# Use the model to make predictions
predictions = model.predict(input_padded)

# Interpret the predictions
for i, code in enumerate(vul_code_1):
    prediction = np.round(predictions[i][0],2)
    print(prediction)
    if prediction > 0.5:
        print(f"Code:\n{code}\nPredicted: Positive (Bug)\n")
    else:
        print(f"Code:\n{code}\nPredicted: Negative (No Bug)\n")

import numpy as np
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense
from keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Sample text data and corresponding labels
texts = df['code_without_comments']
labels = df['bug']  # 1 for positive, 0 for negative

# Tokenize the text data
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

# Pad sequences to a fixed length (adjust maxlen as needed)
maxlen = 10
X = pad_sequences(sequences, maxlen=maxlen)

# Convert labels to numpy array
y = np.array(labels)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the CNN model
model = Sequential()
model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=maxlen))
model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification

# Compile the model
optimizer = Adam(learning_rate=0.001)
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

# Train the model on the training data
model.fit(X_train, y_train, epochs=10, batch_size=16)

# Evaluate the model on the test data
y_pred = model.predict(X_test)
y_pred_binary = (y_pred > 0.5).astype(int)

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred_binary)
precision = precision_score(y_test, y_pred_binary)
recall = recall_score(y_test, y_pred_binary)
f1 = f1_score(y_test, y_pred_binary)

print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1-Score: {f1:.2f}")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
conf_matrix = confusion_matrix(y_test, y_pred_binary)
print("Confusion Matrix:")
print(conf_matrix)

model.save("cnn_b16_full.h5")

df.head()

# Make predictions (you can replace with your test data)
test_texts = [df['code_without_comments'][141665]]
test_sequences = tokenizer.texts_to_sequences(test_texts)
X_test = pad_sequences(test_sequences, maxlen=maxlen)
predictions = model.predict(X_test)

# Print the predictions
for i, text in enumerate(test_texts):
    sentiment = "True" if predictions[i] > 0.5 else "False"
    print(f"Text: {text}\nPredicted Sentiment: {sentiment}\n")

import numpy as np
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout
from keras.optimizers import Adam

# Sample text data and corresponding labels
texts = df['code_without_comments']
labels = df['bug'] # 1 for positive, 0 for negative

# Tokenize the text data
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

# Pad sequences to a fixed length (adjust maxlen as needed)
maxlen = 10
X = pad_sequences(sequences, maxlen=maxlen)

# Convert labels to numpy array
y = np.array(labels)

# Define the LSTM model
model = Sequential()
model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=maxlen))
model.add(LSTM(128, return_sequences=True))
model.add(LSTM(64))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))  # Optional dropout for regularization
model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification

# Compile the model
optimizer = Adam(learning_rate=0.001)
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

# Train the model
model.fit(X, y, epochs=10, batch_size=16)

# Make predictions (you can replace with your test data)
# Remember to preprocess your test data in the same way as the training data.

# Evaluate the model
loss, accuracy = model.evaluate(X, y)
print(f"Training Accuracy: {accuracy * 100:.2f}%")

#with train test split
import numpy as np
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout
from keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Sample text data and corresponding labels
texts = df['code_without_comments']
labels = df['bug'] # 1 for positive, 0 for negative

# Tokenize the text data
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

# Pad sequences to a fixed length (adjust maxlen as needed)
maxlen = 10
X = pad_sequences(sequences, maxlen=maxlen)

# Convert labels to numpy array
y = np.array(labels)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the LSTM model
model = Sequential()
model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=maxlen))
model.add(LSTM(128, return_sequences=True))
model.add(LSTM(64))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))  # Optional dropout for regularization
model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification

# Compile the model
optimizer = Adam(learning_rate=0.001)
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

# Train the model on the training data
model.fit(X_train, y_train, epochs=10, batch_size=16)

# Evaluate the model on the test data
y_pred = model.predict(X_test)
y_pred_binary = (y_pred > 0.5).astype(int)

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred_binary)
precision = precision_score(y_test, y_pred_binary)
recall = recall_score(y_test, y_pred_binary)
f1 = f1_score(y_test, y_pred_binary)

print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1-Score: {f1:.2f}")

# Make predictions (you can replace with your test data)
test_texts = [df['code_without_comments'][50480]]
test_sequences = tokenizer.texts_to_sequences(test_texts)
X_test = pad_sequences(test_sequences, maxlen=maxlen)
predictions = model.predict(X_test)

# Print the predictions
for i, text in enumerate(test_texts):
    sentiment = "True" if predictions[i] > 0.5 else "False"
    print(f"Text: {text}\nPredicted Bug status: {sentiment}\n")

# Example input code for testing


# Tokenize and pad the input code
input_sequences = tokenizer.texts_to_sequences(vul_code)
input_padded = pad_sequences(input_sequences, maxlen=maxlen)

# Use the model to make predictions
predictions = model.predict(input_padded)

# Interpret the predictions
for i, code in enumerate(input_code):
    prediction = np.round(predictions[i][0],2)
    print(prediction)
    if prediction >= 0.5:
        print(f"Code:\n{code}\nPredicted: Positive (Bug)\n")
    else:
        print(f"Code:\n{code}\nPredicted: Negative (No Bug)\n")

model.save("lstm_b16_full.h5")

import numpy as np
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Bidirectional, LSTM, Flatten, Dense
from keras.optimizers import Adam

texts = df['code_without_comments']
labels = df['bug']

# Tokenize the text data
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

maxlen = 10
X = pad_sequences(sequences, maxlen=maxlen)

y = np.array(labels)

# Define the BiLSTM model
model = Sequential()
model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=maxlen))
model.add(Bidirectional(LSTM(64, return_sequences=True)))  # You can adjust the number of LSTM units as needed
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

optimizer = Adam(learning_rate=0.001)
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

# Train the model
model.fit(X, y, epochs=10, batch_size=16)

import numpy as np
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Bidirectional, Dense, Dropout
from keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from keras.layers import Embedding, LSTM, Dense, Dropout
# Sample text data and corresponding labels
texts = df['code_without_comments']
labels = df['bug'] # 1 for positive, 0 for negative

# Tokenize the text data
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

# Pad sequences to a fixed length (adjust maxlen as needed)
maxlen = 10
X = pad_sequences(sequences, maxlen=maxlen)

# Convert labels to numpy array
y = np.array(labels)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the BiLSTM model
model = Sequential()
model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=maxlen))
model.add(Bidirectional(LSTM(128, return_sequences=True)))
model.add(Bidirectional(LSTM(64)))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))  # Optional dropout for regularization
model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification

# Compile the model
optimizer = Adam(learning_rate=0.001)
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

# Train the model on the training data
model.fit(X_train, y_train, epochs=10, batch_size=16)

# Evaluate the model on the test data
y_pred = model.predict(X_test)
y_pred_binary = (y_pred > 0.5).astype(int)

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred_binary)
precision = precision_score(y_test, y_pred_binary)
recall = recall_score(y_test, y_pred_binary)
f1 = f1_score(y_test, y_pred_binary)

print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1-Score: {f1:.2f}")

# Make predictions (you can replace with your test data)
test_texts = [df['code_without_comments'][50480]]
test_sequences = tokenizer.texts_to_sequences(test_texts)
X_test = pad_sequences(test_sequences, maxlen=maxlen)
predictions = model.predict(X_test)

# Print the predictions
for i, text in enumerate(test_texts):
    sentiment = "True" if predictions[i] > 0.5 else "False"
    print(f"Text: {text}\nPredicted Bug status: {sentiment}\n")

"""2nd approach

"""

df

new_df=df[:1000]

import nltk
nltk.download('punkt')

import nltk
nltk.download('stopwords')

import pandas as pd
from sklearn.utils import shuffle

# Shuffle the original DataFrame
shuffled_df = shuffle(df, random_state=42)

# Select the first 1000 rows to create a new DataFrame
new_df = shuffled_df.head(10000).copy()

new_df['bug'].value_counts()

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import string

# Sample dataset


# Define a function to preprocess the code
def preprocess_code(code):
    # Tokenize the code
    tokens = word_tokenize(code)

    # Remove punctuation and convert to lowercase
    tokens = [word.lower() for word in tokens if word.isalnum()]

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]

    # Stem the words
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(word) for word in tokens]

    # Join the tokens back into a string
    preprocessed_code = ' '.join(tokens)

    return preprocessed_code

# Apply the preprocessing function to the 'code' column
new_df['preprocessed_code'] = new_df['code'].apply(preprocess_code)

# Now you can use the 'preprocessed_code' column for TF-IDF or Word2Vec embedding

new_df

new_df['preprocessed_code'][0]

from gensim.models import Word2Vec

# Tokenize the preprocessed code snippets
tokenized_code = new_df['preprocessed_code'].apply(lambda x: x.split())

# Train a Word2Vec model
model = Word2Vec(tokenized_code, vector_size=100, window=5, min_count=1, sg=0)

import numpy as np

def embed_code(tokens, model):
    vec = np.zeros(model.vector_size)
    for token in tokens:
        if token in model.wv:
            vec += model.wv[token]
    if len(tokens) > 0:
        vec /= len(tokens)
    return vec

# Apply the embed_code function to your preprocessed code snippets
new_df['code_embedding'] = tokenized_code.apply(lambda x: embed_code(x, model))

new_df

new_df_encoded = pd.get_dummies(new_df, columns=['flaw'], drop_first=True)

new_df_encoded

new_df

new_df_encoded = pd.get_dummies(new_df, columns=['flaw'], drop_first=True)

# Apply one-hot encoding to the 'flaw' column and replace it in the DataFrame
new_df = pd.get_dummies(new_df, columns=['flaw'], drop_first=True)

new_df

new_df.drop(columns=['code', 'preprocessed_code'], inplace=True)

new_df['bug'].value_counts()

new_df.drop('flaw_CWE-121',axis=1,inplace=True)

new_df

# Verify the column name and apply one-hot encoding
if 'flaw' in new_df.columns:
    new_df = pd.get_dummies(new_df, columns=['flaw'], drop_first=True)

new_df

from sklearn.model_selection import train_test_split

X = new_df['code_embedding']  # Input feature
y = new_df['bug']  # Target variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.linear_model import LogisticRegression

# Initialize and train the Logistic Regression model
clf = LogisticRegression()
clf.fit(X_train.tolist(), y_train.tolist())

# Make predictions on the test data
y_pred = clf.predict(X_test.tolist())

# Evaluate the model
from sklearn.metrics import accuracy_score, classification_report

accuracy = accuracy_score(y_test.tolist(), y_pred)
report = classification_report(y_test.tolist(), y_pred)

print("Accuracy:", accuracy)
print("Classification Report:\n", report)

from sklearn.svm import SVC

# Initialize and train the SVM classifier
svm_classifier = SVC(kernel='linear', random_state=42)
svm_classifier.fit(X_train.tolist(), y_train.tolist())

# Make predictions on the test data
y_pred_svm = svm_classifier.predict(X_test.tolist())

# Evaluate the SVM model
from sklearn.metrics import accuracy_score, classification_report

accuracy_svm = accuracy_score(y_test.tolist(), y_pred_svm)
report_svm = classification_report(y_test.tolist(), y_pred_svm)

print("SVM Accuracy:", accuracy_svm)
print("SVM Classification Report:\n", report_svm)



# Preprocess the input code snippet (assuming you have a variable 'input_code' containing the snippet)
input_code = df['code'][141665]

preprocessed_input_code = preprocess_code(input_code)

# Convert the preprocessed input code to the same feature format (e.g., Word2Vec)
input_code_embedding = embed_code(preprocessed_input_code, model)  # Use your Word2Vec model

# Reshape the input_code_embedding into a 2D NumPy array
input_code_embedding = np.array(input_code_embedding).reshape(1, -1)

# Use the trained SVM model to make a prediction
prediction = svm_classifier.predict(input_code_embedding)
print(prediction)

# Check the prediction
if prediction == 1:
    print("The SVM model predicts a bug in the input code snippet.")
else:
    print("The SVM model predicts no bug in the input code snippet.")

