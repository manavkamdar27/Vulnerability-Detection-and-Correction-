# -*- coding: utf-8 -*-
"""FYP_SQLI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eMs8sALv5PLTGe8iejfpX9aUfNVIjRjf
"""

import glob
import time
import pandas as pd
from nltk import ngrams
from nltk.tokenize import sent_tokenize
import nltk
import keras
import os
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.stem import PorterStemmer
from nltk.stem import PorterStemmer
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

#Preprocess ( all files common format )
def clean_sqli_data(data):

    for i in range(len(data)):

        data[i]=data[i].replace('\n', '')
        data[i]=data[i].replace('%20', ' ')
        data[i]=data[i].replace('=', ' = ')
        data[i]=data[i].replace('((', ' (( ')
        data[i]=data[i].replace('))', ' )) ')
        data[i]=data[i].replace('(', ' ( ')
        data[i]=data[i].replace(')', ' ) ')

    return data

path = "/content/sqli.txt"
sql_lines = []
f = open(path,"r")
for x in f:
  sql_lines.append(x)

sql_lines = clean_sqli_data(sql_lines)
print(sql_lines[:10])

path = "/content/camo.txt"
sql_lines_camo = []
f = open(path,"r")
for x in f:
  sql_lines_camo.append(x)

sql_lines_camo = clean_sqli_data(sql_lines_camo)
print(sql_lines_camo[:10])

path = "/content/libinjection_bypasses.txt"
sql_lines_bp = []
f = open(path,"r")
for x in f:
  sql_lines_bp.append(x)
for i in range(len(sql_lines_bp)):
    sentence=sql_lines_bp[i]
    sql_lines_bp[i]=sentence.split(':')[1]

sql_lines_bp = clean_sqli_data(sql_lines_bp)
print(sql_lines_bp[:10])

path = "/content/sqli1.txt"
sql_lines_owasp = []
f = open(path,"r")
for x in f:
  sql_lines_owasp.append(x)

sql_lines_owasp = clean_sqli_data(sql_lines_owasp)
print(sql_lines_owasp)

path = "/content/Generic_SQLi.txt"
sql_lines_gen = []
f = open(path,"r")
for x in f:
  sql_lines_gen.append(x)

sql_lines_gen = clean_sqli_data(sql_lines_gen)
print(sql_lines_gen[:10])

#remove stopwords
stop_words = set(stopwords.words('english'))

def remove_stop_words(posts):

    filtered=''

    for x in posts.split(' '):
        if x not in stop_words:
            filtered+=' '+x

    return filtered

path = "/content/plain.txt"
df = pd.read_csv(os.path.join(path),sep="ManPrekSid",names=['benign'],header=None,engine = 'python')

df.head()

pt = df['benign'].values
pt = pt[:-22]
print(pt)
data = ''
for x in pt:
  data+=" " + x
print(data)

data=remove_stop_words(data)
data=data.split('.')

for i in range(len(data)):
    data[i]=data[i].replace('<', ' <')
    data[i]=data[i].replace('>', '> ')
    data[i]=data[i].replace('=', ' = ')

path='/content/benign_for_training.txt'
benign_data=[]
f = open(path, "r")
for x in f:
    benign_data.append(x)

path='/content/sqli_for_training.txt'
sqli_data=[]
f = open(path, "r")
for x in f:
    sqli_data.append(x)

benign_sentence=[]
for i in benign_data:
    sentences=i.split('.')

    for sentence in sentences:
        benign_sentence.append(sentence)

all_sqli_sentence = sql_lines_owasp + sql_lines_camo + sql_lines_bp + sql_lines_gen + sql_lines
len(all_sqli_sentence)

def optional_numeric_to_numeric(all_sqli_sentence):

    for i in range(len(all_sqli_sentence)):

        all_sqli_sentence[i]=all_sqli_sentence[i].replace('1 ', 'numeric')
        all_sqli_sentence[i]=all_sqli_sentence[i].replace(' 1', 'numeric')
        all_sqli_sentence[i]=all_sqli_sentence[i].replace("'1 ", "'numeric ")
        all_sqli_sentence[i]=all_sqli_sentence[i].replace(" 1'", " numeric'")
        all_sqli_sentence[i]=all_sqli_sentence[i].replace('1,', 'numeric,')
        all_sqli_sentence[i]=all_sqli_sentence[i].replace("1\ ", 'numeric')
        all_sqli_sentence[i]=all_sqli_sentence[i].replace("‘1", '‘numeric')
        all_sqli_sentence[i]=all_sqli_sentence[i].replace(" 2 ", " numeric ")
        all_sqli_sentence[i]=all_sqli_sentence[i].replace(' 3 ', ' numeric ')
        all_sqli_sentence[i]=all_sqli_sentence[i].replace(' 3--', ' numeric--')
        all_sqli_sentence[i]=all_sqli_sentence[i].replace(" 4 ", ' numeric ')
        all_sqli_sentence[i]=all_sqli_sentence[i].replace(" 5 ", ' numeric ')
        all_sqli_sentence[i]=all_sqli_sentence[i].replace(' 6 ', ' numeric ')
        all_sqli_sentence[i]=all_sqli_sentence[i].replace(" 7 ", ' numeric ')
        all_sqli_sentence[i]=all_sqli_sentence[i].replace(" 8 ", ' numeric ')
        all_sqli_sentence[i]=all_sqli_sentence[i].replace('1234', ' numeric ')
        all_sqli_sentence[i]=all_sqli_sentence[i].replace("22", ' numeric ')
        all_sqli_sentence[i]=all_sqli_sentence[i].replace(" 8 ", ' numeric ')
        all_sqli_sentence[i]=all_sqli_sentence[i].replace(" 200 ", ' numeric ')
        all_sqli_sentence[i]=all_sqli_sentence[i].replace("23 ", ' numeric ')
        all_sqli_sentence[i]=all_sqli_sentence[i].replace('"1', '"numeric')
        all_sqli_sentence[i]=all_sqli_sentence[i].replace('1"', '"numeric')
        all_sqli_sentence[i]=all_sqli_sentence[i].replace("7659", 'numeric')
        all_sqli_sentence[i]=all_sqli_sentence[i].replace(" 37 ", ' numeric ')
        all_sqli_sentence[i]=all_sqli_sentence[i].replace(" 45 ", ' numeric ')

    return all_sqli_sentence

all_sqli_sentence

values=[]
for i in all_sqli_sentence:
    values.append((i,1))

for i in data:
    values.append((i,0))

for i in benign_sentence:
    values.append((i,0))

for i in sqli_data:
    values.append((i,1))

values[:5]

len(values)

df=pd.DataFrame(values,columns=['Sentence','Label'])
df.head()

df.to_csv('sqli.csv', index=False, encoding='utf-16')

df=pd.read_csv('sqli.csv',encoding='utf-16')

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer( min_df=2, max_df=0.7,max_features=4096,stop_words=stopwords.words('english'))
posts = vectorizer.fit_transform(df['Sentence'].values.astype('U')).toarray()

posts.shape
posts.shape = (5604,64,64,1)

X=posts

y=df['Label']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

trainX=X_train.copy()
trainX.shape=(X_train.shape[0],trainX.shape[1]*trainX.shape[2])
testX=X_test.copy()
testX.shape=(testX.shape[0],testX.shape[1]*testX.shape[2])

from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()

gnb.fit(trainX, y_train)

pred_gnb = gnb.predict(testX)

X_train.shape

pip install keras==2.12.0

import tensorflow as tf
from keras.models import Sequential
from keras import layers
from keras.preprocessing.text import Tokenizer
from keras.wrappers.scikit_learn import KerasClassifier

model=tf.keras.models.Sequential([

    tf.keras.layers.Conv2D(64, (3,3), activation=tf.nn.relu, input_shape=(64,64,1)),
    tf.keras.layers.MaxPooling2D(2,2),

    tf.keras.layers.Conv2D(128,(3,3), activation=tf.nn.relu),
    tf.keras.layers.MaxPooling2D(2,2),

    tf.keras.layers.Conv2D(256,(3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),

    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(128,activation='relu'),
    tf.keras.layers.Dense(64, activation=tf.nn.relu),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
model.summary()

classifier_nn = model.fit(X_train,y_train,
                    epochs=10,
                    verbose=True,
                    validation_data=(X_test, y_test),
                    batch_size=128)

pred=model.predict(X_test)

for i in range(len(pred)):
    if pred[i]>0.5:
        pred[i]=1
    elif pred[i]<=0.5:
        pred[i]=0

from sklearn.metrics import accuracy_score
accuracy_score(y_test, pred)

for i,j in zip(y_test,pred):
    print(i==j)

from keras.models import load_model
import pickle

model.save('my_model_cnn.h5')
with open('vectorizer_cnn', 'wb') as fin:
    pickle.dump(vectorizer, fin)

def accuracy_function(tp,tn,fp,fn):

    accuracy = (tp+tn) / (tp+tn+fp+fn)

    return accuracy
def precision_function(tp,fp):

    precision = tp / (tp+fp)

    return precision
def recall_function(tp,fn):

    recall=tp / (tp+fn)

    return recall

def confusion_matrix(truth,predicted):

    true_positive = 0
    true_negative = 0
    false_positive = 0
    false_negative = 0

    for true,pred in zip(truth,predicted):

        if true == 1:
            if pred == true:
                true_positive += 1
            elif pred != true:
                false_negative += 1

        elif true == 0:
            if pred == true:
                true_negative += 1
            elif pred != true:
                false_positive += 1

    accuracy=accuracy_function(true_positive, true_negative, false_positive, false_negative)
    precision=precision_function(true_positive, false_positive)
    recall=recall_function(true_positive, false_negative)

    return (accuracy,
            precision,
           recall)

accuracy,precision,recall=confusion_matrix(y_test,pred)

print(" For CNN \n Accuracy : {0} \n Precision : {1} \n Recall : {2}".format(accuracy, precision, recall))

accuracy,precision,recall=confusion_matrix(y_test,pred_gnb)
print(" For Naive Bayes Accuracy : {0} \n Precision : {1} \n Recall : {2}".format(accuracy, precision, recall))

def clean_data(input_val):

    input_val=input_val.replace('\n', '')
    input_val=input_val.replace('%20', ' ')
    input_val=input_val.replace('=', ' = ')
    input_val=input_val.replace('((', ' (( ')
    input_val=input_val.replace('))', ' )) ')
    input_val=input_val.replace('(', ' ( ')
    input_val=input_val.replace(')', ' ) ')
    input_val=input_val.replace('1 ', 'numeric')
    input_val=input_val.replace(' 1', 'numeric')
    input_val=input_val.replace("'1 ", "'numeric ")
    input_val=input_val.replace(" 1'", " numeric'")
    input_val=input_val.replace('1,', 'numeric,')
    input_val=input_val.replace(" 2 ", " numeric ")
    input_val=input_val.replace(' 3 ', ' numeric ')
    input_val=input_val.replace(' 3--', ' numeric--')
    input_val=input_val.replace(" 4 ", ' numeric ')
    input_val=input_val.replace(" 5 ", ' numeric ')
    input_val=input_val.replace(' 6 ', ' numeric ')
    input_val=input_val.replace(" 7 ", ' numeric ')
    input_val=input_val.replace(" 8 ", ' numeric ')
    input_val=input_val.replace('1234', ' numeric ')
    input_val=input_val.replace("22", ' numeric ')
    input_val=input_val.replace(" 8 ", ' numeric ')
    input_val=input_val.replace(" 200 ", ' numeric ')
    input_val=input_val.replace("23 ", ' numeric ')
    input_val=input_val.replace('"1', '"numeric')
    input_val=input_val.replace('1"', '"numeric')
    input_val=input_val.replace("7659", 'numeric')
    input_val=input_val.replace(" 37 ", ' numeric ')
    input_val=input_val.replace(" 45 ", ' numeric ')

    return input_val

import keras
from keras.models import load_model
import pickle


mymodel = tf.keras.models.load_model('my_model_cnn.h5')
myvectorizer = pickle.load(open("vectorizer_cnn", 'rb'))



def predict_sqli_attack():

    repeat=True

    beautify=''
    for i in range(20):
        beautify+= "="

    print(beautify)
    input_val=input("Give me some data to work on : ")
    print(beautify)


    if input_val== '0':
        repeat=False



    input_val=clean_data(input_val)
    input_val=[input_val]



    input_val=myvectorizer.transform(input_val).toarray()
    input_val.shape=(1,64,64,1)
    result=mymodel.predict(input_val)


    print(beautify)


    if repeat == True:

        if result>0.5:
            print("ALERT :::: This can be SQL injection")


        elif result<=0.5:
            print("It seems to be safe")

        print(beautify)

        predict_sqli_attack()

    elif repeat == False:
        print( " Good Bye ")

predict_sqli_attack()